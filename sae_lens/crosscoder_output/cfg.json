{"apply_b_dec_to_input": false, "d_model": 512, "device": "cpu", "normalize_activations": "none", "reshape_activations": "none", "metadata": {"sae_lens_version": "6.21.0", "sae_lens_training_version": "6.21.0", "dataset_path": "NeelNanda/c4-tokenized-2b", "hook_name": "blocks.0.hook_mlp_out", "model_name": "EleutherAI/pythia-70m", "model_class_name": "HookedTransformer", "hook_head_index": null, "context_size": 128, "seqpos_slice": [null], "model_from_pretrained_kwargs": {"center_writing_weights": false}, "prepend_bos": true, "exclude_special_tokens": false, "sequence_separator_token": "bos", "disable_concat_sequences": false}, "hook_names_out": [], "dtype": "float32", "d_sae": 2048, "d_out": 1024, "hook_names_in": [], "d_in": 1024, "n_output_layers": 2, "n_input_layers": 2, "architecture": "crosscoder"}